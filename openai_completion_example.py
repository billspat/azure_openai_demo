#!/usr/bin/env python

# Example Commandline Azure OpenAI client. 
# see readme.md for requirements and usage

import os, json
import sys
from openai import AzureOpenAI

def init_openai(openai_name, api_key):
  """create an openai client for use with calls.  this small function essentially formats the endpoint. 


  Args:
      openai_name (str): 'name' which is the end part of the hostname to create the endpoint (it's not the whole endpoint)
      api_key (str): secret key generated by the azure openAi service.  

  Returns:
      OpenAI.client: client object
  """ 
  azure_endpoint=f"https://{openai_name}.openai.azure.com/"

  client = AzureOpenAI(
    api_key = api_key,  
    api_version="2023-10-01-preview",
    azure_endpoint = azure_endpoint
    )
    
  return client

def generate_completion(client, 
              deployment_name, 
              prompt,
              token_count = 30):
  """run the AI Model in the deployment

  Args:
      client (AzureOpenAI client): client object created with init_openia
      deployment_name (str): name of the 'deployment' or model in Azure OpenAI studio
      prompt (str): the string to use for deployment
      token_count (int, optional): the approx number of tokens to return in response
  """

  response = client.completions.create(model=deployment_name, 
      prompt=prompt, 
      max_tokens=token_count
      )

  return(response)


def chat(client, deployment_name):

  conversation=[{"role": "system", "content": "You are a helpful assistant."}]

  print("-----------------")
  print(f"this is a chat conversation with azure deployment {deployment_name}.")
  print("Keep it _very_ short. ( do not exect 4096 tokens).")

  while True:
    print("press ctrl+c to end")
    user_input = input("Q:")      
    conversation.append({"role": "user", "content": user_input})

    response = client.chat.completions.create(
        model= deployment_name,
        messages=conversation
    )

    conversation.append({"role": "assistant", "content": response.choices[0].message.content})
    print("\n" + response.choices[0].message.content + "\n")


def load_from_env():
  """ get the specifics for this Azure OpenAI deployment for use with this project
  This reads the keys belows in a file called .env
  """
  from dotenv import load_dotenv
  load_dotenv()
  openai_name = os.getenv("OPENAI_NAME")
  api_key = os.getenv("OPENAI_API_KEY")
  openai_deployment = os.getenv("OPENAI_DEPLOYMENT")
  # openai_engine = os.getenv("OPENAI_ENGINE")
  if not(openai_name and api_key and openai_deployment):
      raise Exception("a required environment setting is missing for openAI")
  return(openai_name,api_key,openai_deployment)

  
  
if __name__ == "__main__":

  # get prompt from the command line
  
  if len(sys.argv) > 2:
    warning("include the prompt for the generative ai after the command", file=sys.stderr)
    print('')
    sys.exit(1)

  prompt = sys.argv[1]

  # get the config for your azure model
  openai_name, api_key, openai_deployment = load_from_env()
  
  # create an openai client using config
  client = init_openai(openai_name, api_key)
  
  if prompt == 'chat':
    # magic keyword to start open ended chat
    chat(client, openai_deployment)

  else:
  # use the client to generate a response
    openai_response = generate_completion(client, openai_deployment, prompt)

    # get the first text completion from the openai response object
    if openai_response.choices:
      completion = openai_response.choices[0].text
  
    # print(f"AI completion:  {prompt}...")
    print(f"{prompt}: {completion}")

  
